# preprocess_sequences.py
import polars as pl
import numpy as np
import joblib
import json
import os
from tqdm import tqdm
import gc
from deepctr_torch.inputs import SparseFeat, DenseFeat

# --- è¨­å®š (keiba_analysis.py, evaluate.py ã¨åˆã‚ã›ã‚‹) ---
INPUT_DATA_PATH = "features_engineered_pathB.csv"
PREPROCESSOR_BASE_DIR = "lstm_preprocessor_score"  # ãƒ­ãƒ¼ãƒ‰ç”¨
MODEL_OUTPUT_DIR = "lstm_models_score"
OUTPUT_NPZ_PATH = "preprocessed_lstm_data.npz"  # å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«å
N_STEPS = 5
TARGET_COL = "rank_score"

# å›åç‡è¨ˆç®—ã«å¿…è¦ãªãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿åˆ— (å…ƒãƒ‡ãƒ¼ã‚¿ã«å­˜åœ¨ã™ã‚‹ã“ã¨ã‚’ç¢ºèª)
META_COLS_FOR_EVAL = [
    "race_id",
    "é¦¬å",
    "æ—¥ä»˜",
    "å˜å‹ã‚ªãƒƒã‚º_å®Ÿ",
    "è¤‡å‹åœå†…_å®Ÿ",
    "å˜å‹",  # å˜å‹çš„ä¸­ãƒ•ãƒ©ã‚° (evaluate.py ã§ä½¿ç”¨)
    "å˜å‹ã‚ªãƒƒã‚º",  # ãƒ¬ãƒ¼ã‚¹å‰ã®å˜å‹ã‚ªãƒƒã‚º (evaluate.py ã§ä½¿ç”¨)
    "ç€é †",  # è¿½åŠ 
    "é¦¬ç•ª",  # è¿½åŠ 
    "é¦¬é€£",  # è¿½åŠ  (é¦¬é€£é…å½“é‡‘)
    "é¦¬å˜",  # è¿½åŠ  (é¦¬å˜é…å½“é‡‘)
    "ï¼“é€£è¤‡",  # è¿½åŠ  (ä¸‰é€£è¤‡é…å½“é‡‘)
    "ï¼“é€£å˜",  # è¿½åŠ  (ä¸‰é€£å˜é…å½“é‡‘)
]


# --- ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•° ---
def apply_mappings_pl(df, col, mapping):
    """Polars DataFrameã«ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’é©ç”¨ (æ—§ãƒãƒ¼ã‚¸ãƒ§ãƒ³äº’æ›ã® map_elements ã‚’ä½¿ç”¨)"""
    mapping_filled = {str(k): int(v) for k, v in mapping.items()}
    # map_elements ã‚’ä½¿ã£ã¦è¾æ›¸ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’å®Ÿè¡Œ
    return df.with_columns(
        pl.col(col)
        .cast(pl.Utf8)
        .map_elements(
            lambda x: mapping_filled.get(
                x, 0
            ),  # mappingã«ã‚­ãƒ¼ãŒã‚ã‚Œã°å€¤ã‚’ã€ãªã‘ã‚Œã°ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤(0)ã‚’è¿”ã™
            return_dtype=pl.Int32,  # å‡ºåŠ›ã®å‹ã‚’æŒ‡å®š
        )
        .alias(col)
    )


# --- ãƒ¡ã‚¤ãƒ³å‡¦ç† ---
def main():
    print("--- Polarsã«ã‚ˆã‚‹äº‹å‰å‡¦ç†ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã‚’é–‹å§‹ ---")
    print(f"å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«: {INPUT_DATA_PATH}")
    print(f"è¨­å®š N_STEPS: {N_STEPS}")

    # --- 1. å‰å‡¦ç†ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®ãƒ­ãƒ¼ãƒ‰ ---
    print(f"--- å‰å‡¦ç†ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ '{PREPROCESSOR_BASE_DIR}' ã‹ã‚‰ãƒ­ãƒ¼ãƒ‰ä¸­ ---")
    try:
        scaler = joblib.load(
            os.path.join(PREPROCESSOR_BASE_DIR, "numerical_scaler.pkl")
        )
        with open(
            os.path.join(PREPROCESSOR_BASE_DIR, "categorical_mappings.json"),
            "r",
            encoding="utf-8",
        ) as f:
            all_mappings = json.load(f)

        # â–¼â–¼â–¼ã€ã“ã“ã‹ã‚‰ä¿®æ­£ã€‘â–¼â–¼â–¼
        # 'feature_columns.pkl' ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¦ã€ãƒ¢ãƒ‡ãƒ«ãŒæœŸå¾…ã™ã‚‹æ­£ã—ã„é †åºã‚’å–å¾—ã™ã‚‹
        all_feature_columns = joblib.load(
            os.path.join(
                MODEL_OUTPUT_DIR, "feature_columns.pkl"
            )  # MODEL_OUTPUT_DIR ã‚’å‚ç…§
        )

        # pkl ã‹ã‚‰ãƒ­ãƒ¼ãƒ‰ã—ãŸãƒªã‚¹ãƒˆã¯ã€åå‰ã ã‘ã®ã‚»ãƒƒãƒˆã«å¤‰æ›
        static_name_set = set(
            joblib.load(os.path.join(PREPROCESSOR_BASE_DIR, "static_feature_names.pkl"))
        )
        sequence_name_set = set(
            joblib.load(
                os.path.join(PREPROCESSOR_BASE_DIR, "sequence_feature_names.pkl")
            )
        )

        # all_feature_columns (æ­£ã—ã„é †åº) ã‚’ã€static / sequence ã«æŒ¯ã‚Šåˆ†ã‘ã‚‹
        static_fcs_sorted = [
            fc for fc in all_feature_columns if fc.name in static_name_set
        ]
        sequence_fcs_sorted = [
            fc for fc in all_feature_columns if fc.name in sequence_name_set
        ]

        # ä½¿ã†ã®ã¯ã€ã“ã®ã€Œåå‰ã®é †åºä»˜ããƒªã‚¹ãƒˆã€
        static_feature_names_ordered = [fc.name for fc in static_fcs_sorted]
        sequence_feature_names_ordered = [fc.name for fc in sequence_fcs_sorted]

        # å…ƒã®ãƒªã‚¹ãƒˆã¯å‰Šé™¤ (ã¾ãŸã¯ä½¿ã‚ãªã„)
        # sequence_feature_names = joblib.load(...) # <-- ä¸è¦
        # static_feature_names = joblib.load(...) # <-- ä¸è¦

        print(
            f"  - Static features (é †åºè£œæ­£æ¸ˆ): {len(static_feature_names_ordered)}ä»¶"
        )
        print(
            f"  - Sequence features (é †åºè£œæ­£æ¸ˆ): {len(sequence_feature_names_ordered)}ä»¶"
        )
        print("å‰å‡¦ç†ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®ãƒ­ãƒ¼ãƒ‰å®Œäº†")
        # â–²â–²â–²ã€ã“ã“ã¾ã§ä¿®æ­£ã€‘â–²â–²â–²

    except Exception as e:
        print(f"ã‚¨ãƒ©ãƒ¼: å‰å‡¦ç†ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        return

    # ç‰¹å¾´é‡ãƒªã‚¹ãƒˆã®æ•´ç† (ã“ã“ã‚‚ä¿®æ­£)
    # â–¼â–¼â–¼ã€ã“ã“ã‹ã‚‰ä¿®æ­£ã€‘â–¼â–¼â–¼
    categorical_features = list(all_mappings.keys())
    numerical_features_static_scaled = scaler.feature_names_in_.tolist()

    # ã‚·ãƒ¼ã‚±ãƒ³ã‚¹å†…ã®æ•°å€¤ç‰¹å¾´é‡ (ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°å¯¾è±¡å¤–)
    sequence_numerical_names = [
        name
        for name in sequence_name_set  # set ã‚’å‚ç…§
        if name not in all_mappings and name != TARGET_COL
    ]

    numerical_features_all = list(
        set(numerical_features_static_scaled + sequence_numerical_names)
    )
    # â–²â–²â–²ã€ã“ã“ã¾ã§ä¿®æ­£ã€‘â–²â–²â–²
    # --- 2. ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã¨åˆæœŸå‡¦ç† (Polars) ---
    print(f"--- ãƒ‡ãƒ¼ã‚¿ ({INPUT_DATA_PATH}) ã‚’ Polars ã§èª­ã¿è¾¼ã¿ä¸­ ---")
    try:
        # å‹æ¨æ¸¬ã‚’å¢—ã‚„ã—ã€æ—¥ä»˜ã‚‚ãƒ‘ãƒ¼ã‚¹
        df_pl = pl.read_csv(
            INPUT_DATA_PATH, try_parse_dates=True, infer_schema_length=10000
        )

        # ã‚½ãƒ¼ãƒˆ (ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ä½œæˆã®å‰æ)
        df_pl = df_pl.sort(["é¦¬å", "æ—¥ä»˜"])

        # è©•ä¾¡ã«å¿…è¦ãªãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿åˆ—ãŒå­˜åœ¨ã™ã‚‹ã‹ç¢ºèª
        missing_meta = [col for col in META_COLS_FOR_EVAL if col not in df_pl.columns]
        if missing_meta:
            print(f"è­¦å‘Š: è©•ä¾¡ã«å¿…è¦ãªãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿åˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {missing_meta}")
            # ã¨ã‚Šã‚ãˆãšç¶šè¡Œã™ã‚‹ãŒã€evaluate.py ã§å•é¡Œã«ãªã‚‹å¯èƒ½æ€§ã‚ã‚Š
            # å¿…è¦ãªã‚‰ãƒ€ãƒŸãƒ¼åˆ—ã‚’è¿½åŠ 
            # for col in missing_meta:
            #     df_pl = df_pl.with_columns(pl.lit(None).alias(col))

    except Exception as e:
        print(f"ã‚¨ãƒ©ãƒ¼: ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã¾ãŸã¯åˆæœŸå‡¦ç†ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        return

    # --- 3. å‰å‡¦ç†ã®é©ç”¨ (Polars) ---
    print("--- å‰å‡¦ç†ã‚’ Polars ã§é©ç”¨ä¸­ ---")
    try:
        # ã‚«ãƒ†ã‚´ãƒªç‰¹å¾´é‡ãƒãƒƒãƒ”ãƒ³ã‚°
        print("  - ã‚«ãƒ†ã‚´ãƒªç‰¹å¾´é‡ã®ãƒãƒƒãƒ”ãƒ³ã‚°...")
        for col in tqdm(categorical_features, desc="Mapping"):
            if col in df_pl.columns:
                mapping = all_mappings.get(col)
                if mapping:
                    df_pl = apply_mappings_pl(df_pl, col, mapping)
                # else: print(f"    - Warning: Mapping not found for {col}") # Debug
            # else: print(f"    - Warning: Column {col} not in df") # Debug

        # å˜å‹ã‚ªãƒƒã‚ºã®å¯¾æ•°å¤‰æ› (å‰å‡¦ç†ã®ä¸€éƒ¨ã¨ã—ã¦ã“ã“ã§å®Ÿæ–½)
        if (
            "å˜å‹ã‚ªãƒƒã‚º" in df_pl.columns
            and "å˜å‹ã‚ªãƒƒã‚º" in numerical_features_static_scaled
        ):
            print("  - å˜å‹ã‚ªãƒƒã‚ºã®å¯¾æ•°(log1p)å¤‰æ›...")
            # inf/-inf/null ã‚’ 0 ã§åŸ‹ã‚ã¦ã‹ã‚‰ log1p
            df_pl = df_pl.with_columns(
                pl.col("å˜å‹ã‚ªãƒƒã‚º")
                .fill_nan(0.0)
                .fill_null(0.0)
                .log1p()
                .alias("å˜å‹ã‚ªãƒƒã‚º")
            )

        # é™çš„æ•°å€¤ç‰¹å¾´é‡ã®ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
        print(
            f"  - é™çš„æ•°å€¤ç‰¹å¾´é‡ ({len(numerical_features_static_scaled)}ä»¶) ã®ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°..."
        )
        static_num_cols_in_df = [
            col for col in numerical_features_static_scaled if col in df_pl.columns
        ]
        if static_num_cols_in_df:
            # ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã®ãŸã‚ã« NumPy ã«å¤‰æ›
            print("    - NumPyã«å¤‰æ›ä¸­...")
            np_data_to_scale = df_pl.select(static_num_cols_in_df).to_numpy()

            # NaN/Infã‚’0ã§ç½®æ› (NumPyå´ã§)
            np_data_to_scale = np.nan_to_num(
                np_data_to_scale, nan=0.0, posinf=0.0, neginf=0.0
            )

            # ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°å®Ÿè¡Œ
            print("    - ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°å®Ÿè¡Œä¸­...")
            scaled_data = scaler.transform(np_data_to_scale)

            # ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°çµæœã‚’ Polars DataFrame ã«æˆ»ã™
            print("    - Polarsã«æˆ»ã—ä¸­...")
            df_scaled = pl.DataFrame(scaled_data, schema=static_num_cols_in_df)

            # å…ƒã®DataFrameã«çµåˆ (å…ƒã®åˆ—ã‚’å‰Šé™¤ã—ã¦çµåˆ)
            df_pl = df_pl.drop(static_num_cols_in_df).hstack(df_scaled)
            print("    - ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°å®Œäº†")
        else:
            print("    - ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°å¯¾è±¡ã®é™çš„æ•°å€¤ç‰¹å¾´é‡ãŒãƒ‡ãƒ¼ã‚¿ã«å­˜åœ¨ã—ã¾ã›ã‚“ã€‚")

        # ãã®ä»–ã®æ•°å€¤ç‰¹å¾´é‡ (ã‚·ãƒ¼ã‚±ãƒ³ã‚¹å†…ãªã©) ã® NaN/Inf å‡¦ç†
        other_num_cols = [
            col
            for col in numerical_features_all
            if col not in numerical_features_static_scaled and col in df_pl.columns
        ]
        if other_num_cols:
            print(
                f"  - ãã®ä»–ã®æ•°å€¤ç‰¹å¾´é‡ ({len(other_num_cols)}ä»¶) ã® NaN/Inf ã‚’ 0 ã§ç½®æ›..."
            )
            df_pl = df_pl.with_columns(
                [
                    pl.col(col).fill_nan(0.0).fill_null(0.0).alias(col)
                    for col in other_num_cols
                ]
            )

        # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆåˆ—ã® NaN/Inf å‡¦ç†
        if TARGET_COL in df_pl.columns:
            print(f"  - ã‚¿ãƒ¼ã‚²ãƒƒãƒˆåˆ— ({TARGET_COL}) ã® NaN/Inf ã‚’ 0 ã§ç½®æ›...")
            df_pl = df_pl.with_columns(
                pl.col(TARGET_COL).fill_nan(0.0).fill_null(0.0).alias(TARGET_COL)
            )

        print("å‰å‡¦ç†é©ç”¨å®Œäº†")

    except Exception as e:
        print(f"ã‚¨ãƒ©ãƒ¼: å‰å‡¦ç†ã®é©ç”¨ä¸­ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        import traceback

        traceback.print_exc()
        return

    # --- 4. ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ãƒ‡ãƒ¼ã‚¿ã®ç”Ÿæˆ (Polars Window Functions) ---
    print("--- ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ãƒ‡ãƒ¼ã‚¿ã‚’ Polars ã§ç”Ÿæˆä¸­ ---")
    try:
        # é¦¬ã”ã¨ã®ãƒ¬ãƒ¼ã‚¹ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ (0å§‹ã¾ã‚Š) ã‚’ç”Ÿæˆ
        # pl.cum_count()ã§ã¯ãªãã€ã‚°ãƒ«ãƒ¼ãƒ—å†…ã®è¡Œç•ªå·ã‚’ç”Ÿæˆã™ã‚‹
        df_pl = df_pl.with_columns(
            pl.int_range(0, pl.len()).over("é¦¬å").alias("_horse_race_idx")
        )  # â† ä¿®æ­£

        # éå»Nèµ°åˆ†ã®ãƒ‡ãƒ¼ã‚¿ã‚’å‚ç…§ã™ã‚‹å¼ã‚’ç”Ÿæˆ
        lag_exprs = []
        for i in range(1, N_STEPS + 1):
            for col_name in sequence_feature_names_ordered:
                if col_name in df_pl.columns:
                    lag_exprs.append(
                        pl.col(col_name)
                        .shift(i)
                        .over("é¦¬å")
                        .alias(f"{col_name}_lag_{i}")
                    )
                # else: print(f"    - Warning: Seq col {col_name} not found, skipping lag {i}") # Debug

        # Shiftã‚’å®Ÿè¡Œã—ã¦æ–°ã—ã„åˆ—ã‚’è¿½åŠ 
        print(f"  - Shift ({len(lag_exprs)}åˆ—) ã‚’å®Ÿè¡Œä¸­...")
        df_pl = df_pl.with_columns(lag_exprs)

        # N_STEPS åˆ†ã®éå»ãƒ‡ãƒ¼ã‚¿ãŒãªã„è¡Œã‚’é™¤å¤–
        print(f"  - N_STEPS ({N_STEPS}) æœªæº€ã®è¡Œã‚’é™¤å¤–ä¸­...")
        original_rows = len(df_pl)
        df_sequences = df_pl.filter(pl.col("_horse_race_idx") >= N_STEPS)
        print(f"    - {original_rows} -> {len(df_sequences)} è¡Œã«ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°")

        # ä¸è¦ã«ãªã£ãŸlagæœŸé–“ä¸­ã®è¡Œã¨ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹åˆ—ã‚’å‰Šé™¤ (ãƒ¡ãƒ¢ãƒªç¯€ç´„ã®ãŸã‚æ—©ã‚ã«)
        # df_pl = df_sequences # ãƒ¡ãƒ¢ãƒªã«ä½™è£•ãŒã‚ã‚Œã° df_pl ã‚’æ®‹ã—ã¦ã‚‚è‰¯ã„
        del df_pl  # ãƒ¡ãƒ¢ãƒªè§£æ”¾
        gc.collect()
        df_sequences = df_sequences.drop("_horse_race_idx")

        # ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ãƒ‡ãƒ¼ã‚¿ã‚’ã¾ã¨ã‚ã‚‹
        print("  - ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ãƒ‡ãƒ¼ã‚¿ã‚’ NumPy é…åˆ—ã«å¤‰æ›ä¸­...")
        sequence_data_np = []
        pbar_seq = tqdm(
            sequence_feature_names_ordered, desc="Structuring Sequences"
        )  # é †åºä»˜ããƒªã‚¹ãƒˆã‚’å‚ç…§
        for col_name in pbar_seq:
            lag_cols = [
                f"{col_name}_lag_{i}" for i in range(N_STEPS, 0, -1)
            ]  # lag_5, lag_4, ..., lag_1 ã®é †
            valid_lag_cols = [col for col in lag_cols if col in df_sequences.columns]

            if not valid_lag_cols:
                print(
                    f"    - è­¦å‘Š: ç‰¹å¾´é‡ '{col_name}' ã®lagåˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ã‚¼ãƒ­ã§åŸ‹ã‚ã¾ã™ã€‚"
                )
                # Add zeros array of shape (n_rows, N_STEPS)
                seq_np = np.zeros((len(df_sequences), N_STEPS))
            else:
                # Select the lag columns and convert to NumPy
                # fill_null(0) ã§ãƒ‘ãƒ‡ã‚£ãƒ³ã‚° (shift ã§ç”Ÿã˜ãŸ Null ã‚’ 0 åŸ‹ã‚)
                seq_np = df_sequences.select(
                    [pl.col(c).fill_null(0) for c in valid_lag_cols]
                ).to_numpy()

                # If some lags were missing, pad with zeros
                if len(valid_lag_cols) < N_STEPS:
                    num_missing = N_STEPS - len(valid_lag_cols)
                    # Assuming lags are missing from the oldest (e.g., lag_5), pad at the beginning
                    padding = np.zeros((len(df_sequences), num_missing))
                    seq_np = np.hstack([padding, seq_np])

            # (n_rows, N_STEPS) -> (n_rows, N_STEPS, 1)
            sequence_data_np.append(seq_np[:, :, np.newaxis])

        # (n_features, n_rows, N_STEPS, 1) -> (n_rows, N_STEPS, n_features)
        X_seq_np = np.concatenate(sequence_data_np, axis=2)

        # ãƒ¡ãƒ¢ãƒªè§£æ”¾: lagåˆ—ã‚’å«ã‚€å¯èƒ½æ€§ã®ã‚ã‚‹DataFrameã‚’å‰Šé™¤
        lag_column_names = [expr.meta.output_name() for expr in lag_exprs]
        df_sequences = df_sequences.drop(lag_column_names)
        del sequence_data_np
        gc.collect()

        print("ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆå®Œäº†")
        print(f"  - X_seq_np shape: {X_seq_np.shape}")

    except Exception as e:
        print(f"ã‚¨ãƒ©ãƒ¼: ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ãƒ‡ãƒ¼ã‚¿ã®ç”Ÿæˆä¸­ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        import traceback

        traceback.print_exc()
        return

    # --- 5. é™çš„ãƒ‡ãƒ¼ã‚¿ã€ãƒ©ãƒ™ãƒ«ã€ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’NumPyé…åˆ—ã«å¤‰æ› ---
    print("--- é™çš„ãƒ‡ãƒ¼ã‚¿ã€ãƒ©ãƒ™ãƒ«ã€ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ NumPy é…åˆ—ã«å¤‰æ›ä¸­ ---")
    try:
        # â–¼â–¼â–¼ã€ã“ã“ã‹ã‚‰ä¿®æ­£ã€‘â–¼â–¼â–¼
        # 'static_feature_names_ordered' (æ­£ã—ã„é †åº) ã¨ 'static_name_set' (å…¨ã‚»ãƒƒãƒˆ) ã¯
        # ã‚»ã‚¯ã‚·ãƒ§ãƒ³1 ã§ .pkl ã‹ã‚‰ãƒ­ãƒ¼ãƒ‰/ç”Ÿæˆæ¸ˆã¿ã€‚

        # 'static_feature_names_ordered' ã®ã†ã¡ã€DataFrame (df_sequences) ã« *å®Ÿéš›ã«* å­˜åœ¨ã™ã‚‹åˆ—ã‚’
        # *é †åºã‚’ç¶­æŒã—ãŸã¾ã¾* ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã™ã‚‹ã€‚
        final_static_cols_to_select = [
            col for col in static_feature_names_ordered if col in df_sequences.columns
        ]

        # ãƒã‚§ãƒƒã‚¯: .pkl ã®ã‚»ãƒƒãƒˆã¨ã€DFã«å­˜åœ¨ã™ã‚‹ãƒªã‚¹ãƒˆã‚’æ¯”è¼ƒ
        if len(final_static_cols_to_select) != len(static_name_set):
            print(
                "è­¦å‘Š: .pkl ã® static_feature_names ã‚»ãƒƒãƒˆã¨ DataFrame ã®åˆ—ãŒä¸€è‡´ã—ã¾ã›ã‚“ã€‚"
            )
            print(f"  Expected from pkl (set): {static_name_set}")
            print(f"  Found in DataFrame (list): {set(final_static_cols_to_select)}")

            missing_cols = static_name_set - set(final_static_cols_to_select)
            if missing_cols:
                print(f"  Missing from DataFrame (will not be in NPZ): {missing_cols}")

        # DFã«å­˜åœ¨ã™ã‚‹åˆ— (æ­£ã—ã„é †åº) ã§ NumPy é…åˆ—ã‚’ç”Ÿæˆ
        X_static_np = df_sequences.select(final_static_cols_to_select).to_numpy()
        print(f"  - X_static_np shape: {X_static_np.shape}")
        # â–²â–²â–²ã€ã“ã“ã¾ã§ä¿®æ­£ã€‘â–²â–²â–²

        y_np = df_sequences.select(TARGET_COL).to_numpy().squeeze()  # (n_rows,)
        print(f"  - y_np shape: {y_np.shape}")

        meta_cols_in_df = [
            col for col in META_COLS_FOR_EVAL if col in df_sequences.columns
        ]
        meta_data = df_sequences.select(meta_cols_in_df)
        print(f"  - Meta data shape: {meta_data.shape}")

        # (å…ƒã®ã‚³ãƒ¼ãƒ‰ã¯ã“ã“ã¾ã§)

    except Exception as e:
        print(f"ã‚¨ãƒ©ãƒ¼: NumPyé…åˆ—ã¸ã®å¤‰æ›ä¸­ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        return

    # --- 6. NumPyé…åˆ—ã‚’ .npz ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ ---
    print(f"--- å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã‚’ '{OUTPUT_NPZ_PATH}' ã«ä¿å­˜ä¸­ ---")
    try:
        save_dict = {
            "X_seq": X_seq_np,
            "X_static": X_static_np,
            "y": y_np,
            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚‚åˆ—ã”ã¨ã«ä¿å­˜ (evaluate.py ã§èª­ã¿è¾¼ã¿ã‚„ã™ãã™ã‚‹ãŸã‚)
            **{col: meta_data[col].to_numpy() for col in meta_cols_in_df},
        }

        # â–¼â–¼â–¼ã€ã“ã“ã‹ã‚‰ä¿®æ­£ã€‘â–¼â–¼â–¼
        # ã©ã®é™çš„ç‰¹å¾´é‡ãŒã©ã®åˆ—ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã«å¯¾å¿œã™ã‚‹ã‹ã‚’ä¿å­˜
        # `X_static_np` ã¨ `final_static_cols_to_select` ã®é †åºã¨æ•°ãŒä¸€è‡´
        save_dict["static_feature_names_ordered"] = np.array(
            final_static_cols_to_select  # DFã«å®Ÿåœ¨ã™ã‚‹åˆ—ã®ãƒªã‚¹ãƒˆ (é †åºç¶­æŒ)
        )
        # ã©ã®ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ç‰¹å¾´é‡ãŒã©ã®åˆ—ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã«å¯¾å¿œã™ã‚‹ã‹ã‚’ä¿å­˜
        save_dict["sequence_feature_names_ordered"] = np.array(
            sequence_feature_names_ordered  # ã‚»ã‚¯ã‚·ãƒ§ãƒ³1ã§ãƒ­ãƒ¼ãƒ‰ã—ãŸé †åºä»˜ããƒªã‚¹ãƒˆ
        )
        # â–²â–²â–²ã€ã“ã“ã¾ã§ä¿®æ­£ã€‘â–²â–²â–²

        np.savez_compressed(OUTPUT_NPZ_PATH, **save_dict)
        print("ä¿å­˜å®Œäº†")
    except Exception as e:
        print(f"ã‚¨ãƒ©ãƒ¼: .npz ãƒ•ã‚¡ã‚¤ãƒ«ã®ä¿å­˜ä¸­ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        return

    print(
        "\n" + "=" * 50 + "\nğŸ‰ äº‹å‰å‡¦ç†ãƒ—ãƒ­ã‚°ãƒ©ãƒ ãŒæ­£å¸¸ã«å®Œäº†ã—ã¾ã—ãŸ ğŸ‰\n" + "=" * 50
    )
    print(f"å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«: {OUTPUT_NPZ_PATH}")
    print("ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ evaluate.py ãŠã‚ˆã³ app.py ã‹ã‚‰èª­ã¿è¾¼ã‚“ã§ãã ã•ã„ã€‚")


if __name__ == "__main__":
    main()
